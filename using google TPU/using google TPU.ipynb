{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "# Imports\n",
        "import os, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "import math, re, os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "import cv2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMUwyNdg3t_Q",
        "outputId": "7bf8c123-2712-4755-f66e-b3d77ac8123b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsJ2awhI3KPr",
        "outputId": "02d522c8-d0fd-4e28-ac42-f206f5f3f85d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of accelerators:  1\n"
          ]
        }
      ],
      "source": [
        "try: # detect TPUs\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError: # detect GPUs\n",
        "    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
        "    #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n",
        "\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = [128, 128]\n",
        "EPOCHS = 12\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync"
      ],
      "metadata": {
        "id": "ycBXEbtv3-vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)  # image format uint8 [0,255]\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
        "\n",
        "    return image\n",
        "\n",
        "def read_labeled_tfrecord(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"label\": tf.io.FixedLenFeature([], tf.float32,default_value=0.0),  # shape [] means single element\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    image = decode_image(example['image'])\n",
        "    label = tf.cast(example['label'], tf.float32)\n",
        "    return image, label # returns a dataset of (image, label) pairs\n",
        "\n",
        "def read_unlabeled_tfrecord(example):\n",
        "    UNLABELED_TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n",
        "        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
        "    image = decode_image(example['image'])\n",
        "    idnum = example['id']\n",
        "    return image, idnum # returns a dataset of image(s)\n",
        "\n",
        "def load_dataset(filenames, labeled=True, ordered=False):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
        "\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n",
        "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
        "    return dataset\n",
        "\n",
        "def data_augment(image, label):\n",
        "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n",
        "    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n",
        "    # of the TPU while the TPU itself is computing gradients.\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    #image = tf.image.random_saturation(image, 0, 2)\n",
        "    return image, label\n",
        "\n",
        "def get_training_dataset():\n",
        "    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset\n",
        "\n",
        "def get_validation_dataset(ordered=False):\n",
        "    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "SJxV5ebo3-rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_FILENAMES = tf.io.gfile.glob('./tfrecord/*.tfrec')\n",
        "VALIDATION_FILENAMES = tf.io.gfile.glob('./tfrecord_valid/*.tfrec')"
      ],
      "metadata": {
        "id": "wGhi9gmx3-nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STEPS_PER_EPOCH = 5117// BATCH_SIZE\n",
        "VALIDATION_STEPS = -(-5051 // BATCH_SIZE)\n",
        "with strategy.scope():\n",
        "    #img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.xception.preprocess_input(tf.cast(data, tf.float32)), input_shape=[*IMAGE_SIZE, 3])\n",
        "    #pretrained_model = tf.keras.applications.Xception(weights='imagenet', include_top=False)\n",
        "\n",
        "    img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.vgg16.preprocess_input(tf.cast(data, tf.float32)), input_shape=[*IMAGE_SIZE, 3])\n",
        "    pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False)\n",
        "\n",
        "    pretrained_model.trainable = False # False = transfer learning, True = fine-tuning\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        img_adjust_layer,\n",
        "        pretrained_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(1, activation='softmax')\n",
        "    ])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss = 'BinaryCrossentropy',\n",
        "    metrics=['BinaryAccuracy']\n",
        "    # NEW on TPU in TensorFlow 24: sending multiple batches to the TPU at once saves communications\n",
        "    # overheads and allows the XLA compiler to unroll the loop on TPU and optimize hardware utilization.\n",
        "#     steps_per_execution=16\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOelTi2G3-kI",
        "outputId": "26327dc0-3e99-4092-bc6d-c370b4241c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda (Lambda)             (None, 128, 128, 3)       0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, None, None, 512)   14714688  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 512)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,715,201\n",
            "Trainable params: 513\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(get_training_dataset(), steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS,\n",
        "                    validation_data=get_validation_dataset(), validation_steps=VALIDATION_STEPS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kGwbjgQ5Gj0",
        "outputId": "e29469ad-f897-49b4-aefe-2e85e32716a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "319/319 [==============================] - 35s 63ms/step - loss: 0.7008 - binary_accuracy: 0.4193 - val_loss: 0.6858 - val_binary_accuracy: 0.4215\n",
            "Epoch 2/12\n",
            "319/319 [==============================] - 18s 56ms/step - loss: 0.6838 - binary_accuracy: 0.4187 - val_loss: 0.6940 - val_binary_accuracy: 0.4215\n",
            "Epoch 3/12\n",
            "319/319 [==============================] - 18s 56ms/step - loss: 0.6806 - binary_accuracy: 0.4226 - val_loss: 0.6757 - val_binary_accuracy: 0.4215\n",
            "Epoch 4/12\n",
            "319/319 [==============================] - 18s 57ms/step - loss: 0.6784 - binary_accuracy: 0.4255 - val_loss: 0.6716 - val_binary_accuracy: 0.4215\n",
            "Epoch 5/12\n",
            "319/319 [==============================] - 18s 57ms/step - loss: 0.6733 - binary_accuracy: 0.4158 - val_loss: 0.6705 - val_binary_accuracy: 0.4215\n",
            "Epoch 6/12\n",
            "319/319 [==============================] - 18s 57ms/step - loss: 0.6739 - binary_accuracy: 0.4240 - val_loss: 0.6699 - val_binary_accuracy: 0.4215\n",
            "Epoch 7/12\n",
            "319/319 [==============================] - 18s 57ms/step - loss: 0.6714 - binary_accuracy: 0.4203 - val_loss: 0.6720 - val_binary_accuracy: 0.4215\n",
            "Epoch 8/12\n",
            "319/319 [==============================] - 18s 57ms/step - loss: 0.6707 - binary_accuracy: 0.4250 - val_loss: 0.6687 - val_binary_accuracy: 0.4215\n",
            "Epoch 9/12\n",
            "319/319 [==============================] - 18s 57ms/step - loss: 0.6714 - binary_accuracy: 0.4201 - val_loss: 0.6679 - val_binary_accuracy: 0.4215\n",
            "Epoch 10/12\n",
            "319/319 [==============================] - 18s 57ms/step - loss: 0.6667 - binary_accuracy: 0.4181 - val_loss: 0.6660 - val_binary_accuracy: 0.4215\n",
            "Epoch 11/12\n",
            "319/319 [==============================] - 18s 56ms/step - loss: 0.6718 - binary_accuracy: 0.4254 - val_loss: 0.6656 - val_binary_accuracy: 0.4215\n",
            "Epoch 12/12\n",
            "319/319 [==============================] - 18s 56ms/step - loss: 0.6641 - binary_accuracy: 0.4111 - val_loss: 0.6757 - val_binary_accuracy: 0.4215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qIasHj4J5GgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cx1vzHST5Gc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KDtEGQ_y3-fH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}